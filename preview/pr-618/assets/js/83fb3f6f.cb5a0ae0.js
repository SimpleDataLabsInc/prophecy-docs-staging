"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[51827],{28453:(e,i,r)=>{r.d(i,{R:()=>n,x:()=>c});var s=r(96540);const t={},a=s.createContext(t);function n(e){const i=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function c(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:n(e.components),s.createElement(a.Provider,{value:i},e.children)}},69132:(e,i,r)=>{r.r(i),r.d(i,{assets:()=>o,contentTitle:()=>c,default:()=>d,frontMatter:()=>n,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"administration/fabrics/Spark-fabrics/databricks/databricks","title":"Databricks","description":"Configuring Databricks Fabric","source":"@site/docs/administration/fabrics/Spark-fabrics/databricks/databricks.md","sourceDirName":"administration/fabrics/Spark-fabrics/databricks","slug":"/administration/fabrics/Spark-fabrics/databricks/","permalink":"/prophecy-docs-staging/preview/pr-618/administration/fabrics/Spark-fabrics/databricks/","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"concepts","permalink":"/prophecy-docs-staging/preview/pr-618/tags/concepts"},{"inline":true,"label":"fabric","permalink":"/prophecy-docs-staging/preview/pr-618/tags/fabric"},{"inline":true,"label":"databricks","permalink":"/prophecy-docs-staging/preview/pr-618/tags/databricks"},{"inline":true,"label":"livy","permalink":"/prophecy-docs-staging/preview/pr-618/tags/livy"},{"inline":true,"label":"prophecyManaged","permalink":"/prophecy-docs-staging/preview/pr-618/tags/prophecy-managed"}],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Databricks","id":"databricks","description":"Configuring Databricks Fabric","sidebar_position":2,"tags":["concepts","fabric","databricks","livy","prophecyManaged"]},"sidebar":"adminSidebar","previous":{"title":"Spark fabrics","permalink":"/prophecy-docs-staging/preview/pr-618/administration/fabrics/Spark-fabrics/Fabrics"},"next":{"title":"UC standard cluster support","permalink":"/prophecy-docs-staging/preview/pr-618/administration/fabrics/Spark-fabrics/databricks/ucshared"}}');var t=r(74848),a=r(28453);const n={title:"Databricks",id:"databricks",description:"Configuring Databricks Fabric",sidebar_position:2,tags:["concepts","fabric","databricks","livy","prophecyManaged"]},c=void 0,o={},l=[{value:"Fields",id:"fields",level:2},{value:"Team",id:"team",level:3},{value:"Credentials",id:"credentials",level:3},{value:"Job Sizes",id:"job-sizes",level:3},{value:"Prophecy Library",id:"prophecy-library",level:3},{value:"Artifacts",id:"artifacts",level:3},{value:"Databricks execution",id:"databricks-execution",level:2}];function h(e){const i={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.p,{children:"Create a Databricks fabric to connect Prophecy to your existing Databricks workspace. With a Databricks fabric, you can connect to existing Spark clusters or create new ones, run Spark pipelines, and read or write data, depending on your Databricks permissions."}),"\n",(0,t.jsx)(i.h2,{id:"fields",children:"Fields"}),"\n",(0,t.jsx)(i.p,{children:"Learn about different fields to configure your Databricks Spark fabric."}),"\n",(0,t.jsx)(i.h3,{id:"team",children:"Team"}),"\n",(0,t.jsx)(i.p,{children:"Each fabric is associated with one team. All team members will be able to access the fabric in their projects."}),"\n",(0,t.jsx)(i.h3,{id:"credentials",children:"Credentials"}),"\n",(0,t.jsx)(i.p,{children:"Provide the following information to verify your Databricks credentials."}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Databricks Workspace URL"}),". The URL that points to the workspace that the fabric will use as the execution environment."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Authentication Method"}),". Prophecy supports authentication via ",(0,t.jsx)(i.a,{href:"https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-personal-access-token",children:"Personal Access Token"})," (PAT) and ",(0,t.jsx)(i.a,{href:"/databricks-oauth-authentication",children:"OAuth"}),"."]}),"\n",(0,t.jsx)(i.admonition,{type:"caution",children:(0,t.jsx)(i.p,{children:"Each user in the team will have to authenticate individually using the method you select. An individual user's credentials will determine the level of access they have to Databricks from Prophecy. At minimum, you must have permission to attach clusters in Databricks to use the fabric."})}),"\n",(0,t.jsx)(i.admonition,{type:"note",children:(0,t.jsxs)(i.p,{children:["When using ",(0,t.jsx)(i.strong,{children:"Active Directory"}),", Prophecy takes care of the auto-generation and refreshing of the Databricks personal access tokens. Read more about it ",(0,t.jsx)(i.a,{href:"https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/aad/",children:"here"}),"."]})}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Service Principal Client Secret"})," and ",(0,t.jsx)(i.strong,{children:"Service Principal Client ID"}),". This is required when you use ",(0,t.jsx)(i.a,{href:"/databricks-oauth-authentication/#machine-to-machine-m2m",children:"OAuth for project deployment"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"job-sizes",children:"Job Sizes"}),"\n",(0,t.jsx)(i.p,{children:"Job sizes determine the type of clusters that can be spawned from Prophecy. We recommend using the smallest machines and smallest number of nodes appropriate for your use case."}),"\n",(0,t.jsxs)(i.p,{children:["By default, Prophecy provides one job size that runs on ",(0,t.jsx)(i.a,{href:"https://docs.databricks.com/aws/en/compute#databricks-runtime",children:"Databricks Runtime 14.3"}),". You can edit this job size and add new job sizes."]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{alt:"Job Size configuration",src:r(90041).A+"",width:"2870",height:"1610"})}),"\n",(0,t.jsx)(i.p,{children:"You can either configure the fields with our form, or toggle on the JSON view and copy-paste your compute JSON from Databricks."}),"\n",(0,t.jsxs)(i.p,{children:["The job size configuration mirrors the compute configuration in Databricks. To learn more about compute configuration in Databricks, visit their ",(0,t.jsx)(i.a,{href:"https://docs.databricks.com/aws/en/compute/configure",children:"reference"})," guide."]}),"\n",(0,t.jsx)(i.admonition,{type:"caution",children:(0,t.jsxs)(i.p,{children:["When using Unity Catalog clusters with standard (formerly shared) access mode, note their ",(0,t.jsx)(i.a,{href:"https://docs.databricks.com/en/compute/access-mode-limitations.html#shared-access-mode-limitations-on-unity-catalog",children:"particular limitations"}),". You can see all supported Prophecy features in our ",(0,t.jsx)(i.a,{href:"./ucshared",children:"UC standard cluster support"})," documentation."]})}),"\n",(0,t.jsx)(i.h3,{id:"prophecy-library",children:"Prophecy Library"}),"\n",(0,t.jsxs)(i.p,{children:["Prophecy libraries are Scala and Python libraries that provide additional functionalities on top of Spark. These libraries are installed in your Spark execution environment when you attach to a cluster/create new cluster. You can select the following ",(0,t.jsx)(i.strong,{children:"Resolution Modes"})," to access Prophecy libraries:"]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Public Central"}),". Retrieve Prophecy libraries from the public artifacts."]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["ProphecyLibsScala: ",(0,t.jsx)(i.a,{href:"https://mvnrepository.com/artifact/io.prophecy/prophecy-libs",children:"Maven"})]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["ProphecyLibsPython: ",(0,t.jsx)(i.a,{href:"https://pypi.org/project/prophecy-libs/",children:"PyPI"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"Custom Artifactory"}),". Retrieve Prophecy libraries from an Artifactory URL. You can also download the Prophecy libraries from these links. Note that you can change the version from what the example URL shows."]}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["ProphecyLibsScala: ",(0,t.jsx)(i.code,{children:"https://prophecy-public-bucket.s3.us-east-2.amazonaws.com/prophecy-libs/prophecy-libs-assembly-3.5.0-8.9.0.jar"})]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["ProphecyLibsPython: ",(0,t.jsx)(i.code,{children:"https://prophecy-public-bucket.s3.us-east-2.amazonaws.com/python-prophecy-libs/prophecy_libs-1.9.45-py3-none-any.whl"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"File System"}),". Retrieve Prophecy libraries from your file system. To learn about setting up Prophecy libraries in your Databricks volumes, ",(0,t.jsx)(i.a,{href:"/prophecy-docs-staging/preview/pr-618/engineers/dbx-volumes-plibs",children:"click here"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.admonition,{title:"Whitelist Prophecy libraries",type:"info",children:(0,t.jsxs)(i.p,{children:["To use Prophecy libraries in Databricks environments that have enabled Unity Catalog, you must whitelist the required Maven coordinates or JAR paths. Find instructions ",(0,t.jsx)(i.a,{href:"/engineers/dbx-whitelist-plibs",children:"here"}),"."]})}),"\n",(0,t.jsx)(i.h3,{id:"artifacts",children:"Artifacts"}),"\n",(0,t.jsx)(i.p,{children:"Prophecy supports Databricks volumes. When you run a Python or Scala pipeline via a job, you must bundle them as whl/jar artifacts. These artifacts must then be made accessible to the Databricks job in order to use them as a library installed on the cluster. You can designate a path to a volume for uploading the whl/jar files under Artifacts."}),"\n",(0,t.jsx)(i.h2,{id:"databricks-execution",children:"Databricks execution"}),"\n",(0,t.jsx)(i.p,{children:"To learn about Databricks execution, visit:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/engineers/execution",children:"Interactive Execution"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/prophecy-docs-staging/preview/pr-618/engineers/execution-metrics",children:"Execution Metrics"})}),"\n"]})]})}function d(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},90041:(e,i,r)=>{r.d(i,{A:()=>s});const s=r.p+"assets/images/dbx-job-size-a150c981e85785c5088e93ef4dc3a84e.png"}}]);